# 考前复习

>   注：没有特别注明的地方，回答内容都是来自屁屁踢，有不懂的还可以度娘找解答

[TOC]



------
### 1. 理解操作型与分析型系统分离的必要性

操作型系统擅长事务处理（增删改查），面对大量用户可预知、反复的事务操作；分析型系统擅长读取并分析处理历史、推导的、汇总的值。

数据处理模式上的不同：事务型处理更专注于对数据库联机的日常操作，是对一个或一组记录进行的查询和修改，主要是支持企业的特定应用、保证业务正常运作。人们关注的是响应时间、数据安全性和完整性。而分析型处理是对历史数据进行分析和推理，找出有价值的模式，为决策提供依据，推动业务持续改进。因此使操作型与分析型系统分离，可以提高两个系统的性能。

数据库系统是为已知的任务和负载设计的。数据仓库的查询通常是复杂的,涉及大量汇总级的计算,在数据库系统上处理分析,会大大降低操作任务/业务系统的性能。

<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210621191123912.png" alt="image-20210621191123912" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210621191055749.png" alt="image-20210621191055749" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210621191019005.png" alt="image-20210621191019005" style="zoom:67%;" /></center>

<center>来源：Chap 1, P33 ~ 35</center>



### 2. 理解维度爆炸给数据挖掘带来的困难

数据过高的维度会给计算带来麻烦，在数据挖掘处理时，它会耗费很多的处理时间和内存容量。数据的高维度还使得数据间的关系也不容易察觉，增加了数据的无关属性和噪音点。

当维度增加时，数据在它占据的空间中变得越来越稀疏。点之间的密度和距离的定义，原本是对聚类和异常值检测至关重要的，变得没有意义。

<center>来源：Chap 2, P27</center>

   

### 3. 掌握Minkowski距离，并熟练计算$l_1,l_2,l_\infin$范数

闵氏距离是欧几里得的推广：

   ![image-20210621203742534](%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210621203742534.png)

$l_1$范数：$r=1$时，$dist=\sum_{k=1}^n|p_k-q_k|$（曼哈顿距离，及绝对值之和，$P$与$Q$点的距离为$|x_P-x_Q|+|y_P-y_Q|$）

$l_2$范数：$r=2$时，$dist=\sqrt{\sum_{k=1}^n(p_k-q_k)^2}$（欧式距离） 

$l_\infin$范数：$r=\infin$时，$dist=\max_{1\leq k\leq n}\sum_{k=1}^n|p_k-q_k|$（无穷范数，取向量的最大值）

<center>来源：Chap 2, P39 ~ 43</center>

   

### 4. 理解Hunt's算法的基本过程

将训练记录相继划分为较纯的子集，以递归方式建立决策树。 

令 $Dt$ 为到达节点 $t$ 的训练记录集，类$Y={y_1,y_2,...,y_c}$

递归定义如下： 

– 如果 $Dt$ 都属于同一个类 $yt$，则 $t$ 是标记为 $yt$ 的叶结点 

– 如果 $Dt$ 包含属于多个类的记录，则选择一个属性测试条件，将数据拆分为更小的子集。递归地将过程应用于每个子集。

<center>来源：Chap 3, P17</center>

>   参考：[决策树的起源 - Hunt算法](https://blog.csdn.net/e15273/article/details/79874588)



### 5. 掌握吉尼系数、熵的定义，理解其最大值与最小值的意义

（用于衡量划分是否合理）

**吉尼系数（GINI）**：（不纯度）是一种不等性度量，表示一个随机选中的样本在子集中被分错的可能性。可以用来度量任何不均匀分布；是介于0~1之间的数，总体内包含的类别越杂乱，GINI指数就越大。

对于一个结点 $t$，$GINI(t)=1-\sum_{j=1}^n[p(j/t)]^2$

其中 $p(j/t)$ 是类 $j$ 在结点 $t$ 的频率，$n$ 是类的数量。

最大值：$1-1/n$，当记录平均分布在所有类时，意味着最不感兴趣的信息。

最小值：$0$，当所有记录都属于一个类时，意味着最感兴趣的信息。



**熵（Entropy）**：对随机变量不确定性的度量、信息出现的期望值

$H(X)=\sum_{j=1}^{n}p(j/t)I(j/t)=-\sum_{j=1}^{n}p(j/t)\log_b p(j/t)$

熵只依赖$X$的分布，和$X$的取值没有关系，用来度量不确定性（节点的一致性），当熵越大，$X=X_i$ 的不确定性越大，反之越小。

最大值：$\log_2n$，当记录平均分布在所有类时，意味着最不感兴趣的信息。

最小值：$0$，当所有记录都属于一个类时，意味着最感兴趣的信息。

<center>来源：Chap 3, P30 ~ 38</center>

>   参考：[信息、熵、信息增益、基尼指数](https://blog.csdn.net/ITYTI/article/details/93656655)

### 6. 理解信息增益在构造决策树时的意义

信息增益（Info Gain）：$GAIN_{split}=Entropy(p)-(\sum_{i=1}^k\frac{n_i}{n}Entropy(i))$

信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，在概率中定义为：待分类的集合的熵和选定某个特征的条件熵之差

<center>来源：Chap 3, P39 ~ 40</center>



### 7. 理解最近邻分类的基本思想、近邻分类的三个前提条件与分类过程

基于类比学习，即通过将给定的检验元组与和它相似的训练元组进行比较来学习。当给定一个未知元组时，近邻分类法搜索模式空间，找出最接近未知元组的k个训练元组。这k个训练元祖是未知元组的k个”最近邻“

找到$k$个邻居（最近的点）进行分类

-   计算该样点到其他训练样本的距离
-   找到最近的$k$个邻居
-   用邻居的类标签来确定测试样本的类标签（投票最多的）

三个前提条件：训练集存在内存中；给定距离度量指标；给定k值

<center>来源：Chap 3, P75 ~ 80</center>



### 8. 掌握“拟合不够”与“过拟合”产生的原因

欠拟合：当模型太简单时，训练和测试误差都很大

过拟合：决策树生长过多，在训练集上误差越来越小，测试集上的误差越来越大。训练误差无法再预测分类器在测试集上有怎样的表现。

产生的原因：

1.  噪声导致的
2.  决策树的复杂程度超过了需要的程度
3.  训练误差的减小已经对结果没有更多意义但依然在计算
4.  没有更多的属性来减小样本误差

<center>来源：Chap 3, P47 ~ 49</center>



### 9. 理解并熟练计算分类模型评估指标：Accuracy、Recall、Precision与F1值，并能进行模型对比

<img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210621230535878.png" alt="image-20210621230535878" style="zoom:80%;" />

![image-20210621230410231](%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210621230410231.png)

<center>来源：Chap 3, P66 ~ 67</center>



### 10. 理解随机森林的构造过程，以及它是如何避免过拟合的

**预剪枝**：（生成决策树时）提前停止，在变成完全生长的树之前停止算法。

-   如果所有实例都属于同一个类，则停止
-   如果所有属性值都接近，则停止
-   实例数小于某个用户指定的阈值
-   如果扩展当前节点没有改善杂质质量（例如基尼指数或信息增益）
-   如果实例的类分布与可用功能无关（例如使用卡方测试）

**后剪枝**：决策树发展到完整（过拟合）后，自下而上的方式修剪决策树的节点，如果修剪后泛化错误改进，就用叶节点替换子树，叶节点的类标签由子树中实例的多数类确定。

<center>来源：Chap 3, P51 ~ 53</center>



### 11. 理解关联规则及支持度、置信度的定义，并熟练计算支持度与置信度

关联规则：X、Y都是事务数据集，表示共同出现的频率大小，可以用支持度和置信度度量

支持度（Support）：事务数据库中包含某个项集的事务占事务总数的比例，例如 $X$ 和 $Y$ 共同出现的频率 $\frac{\sigma(X\cup Y)}{|T|}$

置信度（Confidence）：在所有包含 $X$ 的事务中包含 $Y$ 的事务所占比例，$\frac{\sigma(X\cup Y)}{\sigma(X)}$

<center>来源：Chap 4, P2 ~ 4</center>


### 12. 理解Apriori原理及其在频繁项集产生、规则产生时的意义

【原理】如果一个项集是频繁的，那么它的所有子集都是频繁的。相反，如果一个项集是非频繁的，则它的所有超集也一定是非频繁的。

【意义】利用其原理可以通过减少搜索空间，来提高频繁项集逐层产生的效率。

【过程】

1.  令 $K=1$
2.  生成长度为 $k$ 的频繁项集
3.  重复直到没有新的频繁项集被识别
4.  ​    从长度为 $k$ 的频繁项集生成长度为 $k+1$ 的候选项集
5.  ​    修剪包含不频繁的长度为 $k$ 的子集的候选项集
6.  ​    计算每个候选项的支持率
7.  ​    排除频率低的候选项，只留下频率高的候选项

<center>来源：Chap 4, P14 ~ 15</ccenter>



### 13. 给定一个交易数据库与支持度阈值，能熟练运用Apriori算法挖掘频繁项集

1.  第一次扫描，列出一项集，并计数
2.  去除低于阈值的项集，组合出二项集，并计数
3.  重复步骤2，依次组合出N项集，直至项集计数小于阈值，结束

<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623225106774.png" alt="image-20210623225106774" style="zoom:67%;" /></center>



### 14. 理解FP-Growth算法克服了Apriori算法的哪些不足

1.  减少了扫描数据库的次数，只用扫描两次
2.  完整性：包含与挖掘关联规则相关的所有信息
3.  紧凑性：树的高到受到了商品种类数量的限制
4.  候选项变少，不会产生那么大的候选项集

<center>来源：Chap 4, P45</ccenter>



### 15. 给定一个交易数据库与支持度阈值，能熟练运用FP-Growth算法挖掘频繁项集

1.  扫描一次数据集，确定每个项的支持度计数。丢弃非频繁项，将频繁项按照支持度的递减排序，生成频繁项集头表（频率降序排列）
2.  扫描投影，按照频率从大到小的顺序逐条对应写出剔除非频繁项后的频繁集
3.  构建FP树，按照事务ID号的顺序，将处理好的频繁项集映射创建FP树，并在对应节点计数
4.  构建每个对象的条件模式基，建议从频率低的节点（底部）开始
5.  列出表格（如图），剔除低于阈值的项
6.  针对每一项建立条件FP树
7.  找出频繁项集

<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623225533617.png" alt="image-20210623225533617" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623225556601.png" alt="image-20210623225556601" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623225649501.png" alt="image-20210623225649501" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623225717768.png" alt="image-20210623225717768" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623225759689.png" alt="image-20210623225759689" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623230748637.png" alt="image-20210623230748637" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623230909425.png" alt="image-20210623230909425" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623231014685.png" alt="image-20210623231014685" style="zoom:67%;" /></center>
<center><img src="%E8%80%83%E5%89%8D%E5%A4%8D%E4%B9%A0.assets/image-20210623231041684.png" alt="image-20210623231041684" style="zoom:67%;" /></center>

<center>来源：Chap 4, P38 ~ 43</ccenter>



### 16. 理解K-means算法的内容并讨论该算法之不足

过程：

1.  随机选择 $K$ 个对象，每个对象代表一个簇的初始均值或中心
2.  循环开始：
3.  ​    对剩余的每个对象，根据它与簇均值的距离，将他指派到最相似的簇
4.  ​    计算每个簇的新均值
5.  直到准则函数收敛

【不足之处】

1.  只有当簇均值有定义的情况下，K-means 方法才能够使用
2.  用户必须首先给定簇数目
3.  不适合发现非凸形状的簇，或者大小差别很大的簇
4.  对噪声和离群点数据敏感

<center>来源：Chap 5, P19 ~ 40</ccenter>



### 17. 理解凝聚聚类算法的基本过程，并分析该算法的时间与空间复杂度

【过程】

1.  计算距离矩阵
2.  让每个数据点成为一个群集
3.  循环开始：
4.  ​    合并两个距离最近的群集
5.  ​    更新距离矩阵
6.  直到只剩下一个群集

【时间复杂度】$O(N^2)$，【空间复杂度】$O(N^3)$

<center>来源：Chap 4, P44 ~ 63</ccenter>



### 18. 理解DBSCAN算法将待聚类的点分为哪几类，分别解释之

3类：

中心点：中心点领域范围内的点的个数 $\geq$ 临界值

边界点：边界点领域范围内的点个数小于临界值，但是它在中心点领域范围的边界上

噪音点：既不是中性点又不是边界点的点

<center>来源：Chap 4, P71</ccenter>



### 19. 理解DBSCAN算法的思想及它克服了K-means算法哪些不足

【基于密度的聚类】只要一个区域中的点的密度大于某个阈值，就把它加到与之相近的聚类中去，克服基于距离的算法只能发现“类圆形”的聚类的缺点，可发现任意形状的聚类，且对噪声不敏感。但是，其计算密度的时间复杂度大，需要建立空间索引来降低计算量，且对数据维数的伸缩性较差。

【算法步骤】

1.  通过检查数据集中每个对象的 $\epsilon-$领域来寻找聚类
2.  通过一个点 $P$ 的 $\epsilon-$领域包含多于 MinPts（最少包含点数）个对象，则创建一个 $P$ 作为核心对象的新簇
3.   反复地寻找从这个核心对象直接密度可达的对象，这个过程可能涉及一些密度可达簇的合并
4.  当没有新的点可以被添加到任何簇时，过程结束

【比K-means好在哪】

1.  适合发现任意形状的簇
2.  易于发现噪声
3.  无需设置 $K$ 值，只需输入 $\epsilon$、MinPts

<center>来源：Chap 4, P72 ~ 74</ccenter>


### 20. 理解SSE指标对聚类的意义

**S**um of **S**quared **E**rror，错误的平方和

$SSE=\sum_{i=1}^K\sum_{x\in C_i}dist^2(m_i,x)$

【意义】：

1.  可以很好的用于比较两个聚类或两个簇

2.  可以用于估计簇的数目

    增加 $K$ （簇的数目）即可降低SSE
<center>来源：Chap 4, P23</ccenter>

